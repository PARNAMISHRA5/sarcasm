{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":36545,"sourceType":"datasetVersion","datasetId":1309},{"sourceId":2513240,"sourceType":"datasetVersion","datasetId":1517327},{"sourceId":11488662,"sourceType":"datasetVersion","datasetId":7201256}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Depression Detection with Proper Emoji Integration (Fixed Version)\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport emoji\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, Input, Concatenate, Attention\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import BertTokenizer, TFBertModel\nimport gc\nfrom tqdm import tqdm\n\n# Configuration\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nclass DepressionDetector:\n    def __init__(self):\n        self.emoji_to_score = None\n        self.model = None\n        self.tokenizer = None\n        self.max_length = 128\n\n    def load_data(self):\n        emoji_scores = pd.read_csv('/kaggle/input/emoji-sentiment/emoji_sentiment_score.csv')\n        self.emoji_to_score = dict(zip(emoji_scores['Char'], emoji_scores['Sentiment score']))\n\n        # Load and balance datasets\n        dep = pd.read_csv('/kaggle/input/twitter-depression-dataset/clean_d_tweets.csv', usecols=['tweet']).assign(label=1)\n        non_dep = pd.read_csv('/kaggle/input/twitter-depression-dataset/clean_non_d_tweets.csv', usecols=['tweet']).assign(label=0)\n        non_dep = non_dep.sample(n=len(dep), random_state=42)\n\n        # Load sarcasm data\n        sarcasm = pd.concat([chunk[chunk.label == 1] for chunk in\n                           pd.read_csv('/kaggle/input/sarcasm/train-balanced-sarcasm.csv', chunksize=10000)])\n        sarcasm = sarcasm.sample(10000).assign(label=0)\n\n        self.df = pd.concat([\n            dep.rename(columns={'tweet': 'text'}),\n            non_dep.rename(columns={'tweet': 'text'}),\n            sarcasm.rename(columns={'comment': 'text'})\n        ]).sample(frac=1).reset_index(drop=True)  # Reset index for consistency\n\n        # Preprocess\n        self.df['processed'] = self.df.text.apply(self.clean_text)\n        self.df['emoji_score'] = self.df.processed.apply(self.get_emoji_score)\n        self.df['bert_text'] = self.df.processed.apply(emoji.demojize)\n\n    def clean_text(self, text):\n        text = re.sub(r'http\\S+|@\\w+|#', '', str(text))\n        return re.sub(r'\\s+', ' ', text).strip()\n\n    def get_emoji_score(self, text):\n        emojis = [c for c in text if c in emoji.EMOJI_DATA]\n        if not emojis: return 0.5\n        return np.mean([self.emoji_to_score.get(e, 0.5) for e in emojis])\n\n    def extract_bert_features(self):\n        os.makedirs('bert_features', exist_ok=True)\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n\n        for i in tqdm(range(0, len(self.df), 16)):\n            batch = self.df.bert_text.iloc[i:i+16].tolist()\n            inputs = self.tokenizer(batch, return_tensors='tf', padding='max_length',\n                                   truncation=True, max_length=self.max_length)\n            outputs = bert_model(inputs)\n\n            for j, idx in enumerate(range(i, min(i+16, len(self.df)))):  # Use actual indices\n                np.save(f\"bert_features/pooled_{idx}.npy\", outputs.pooler_output[j].numpy())\n                np.savez_compressed(f\"bert_features/sequence_{idx}.npy\", outputs.last_hidden_state[j].numpy())\n\n        del bert_model\n        gc.collect()\n\n    def build_model(self):\n        # Text processing branch\n        sequence_input = Input(shape=(self.max_length, 768), dtype=tf.float16)\n        lstm = Bidirectional(LSTM(128, return_sequences=True))(sequence_input)\n        attention = Attention()([lstm, lstm])\n        text_features = Dense(128)(attention[:, -1, :])\n\n        # Emoji processing branch\n        emoji_input = Input(shape=(1,), dtype=tf.float32)\n        emoji_features = Dense(64, activation='relu')(emoji_input)\n        emoji_features = Dense(32, activation='relu')(emoji_features)\n\n        # Combined features\n        combined = Concatenate()([text_features, emoji_features])\n        combined = Dense(128, activation='relu')(combined)\n        output = Dense(1, activation='sigmoid')(combined)\n\n        self.model = Model(inputs=[sequence_input, emoji_input], outputs=output)\n        self.model.compile(\n            optimizer=tf.keras.mixed_precision.LossScaleOptimizer(Adam(3e-5)),\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, indices, df, batch_size=32):\n        \"\"\"\n        Modified to receive the entire dataframe instead of separate components\n        \"\"\"\n        self.indices = indices\n        self.df = df\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return int(np.ceil(len(self.indices) / self.batch_size))\n\n    def __getitem__(self, idx):\n        # Calculate start and end indices for this batch\n        start_idx = idx * self.batch_size\n        end_idx = min((idx + 1) * self.batch_size, len(self.indices))\n        \n        # Get actual indices for this batch\n        actual_indices = self.indices[start_idx:end_idx]\n        \n        # Load BERT sequence features\n        seq_features = np.stack([np.load(f\"bert_features/sequence_{i}.npy.npz\")['arr_0'] for i in actual_indices])\n        \n        # Get emoji features directly from dataframe using the same indices\n        emoji_features = self.df.iloc[actual_indices].emoji_score.values.reshape(0, 1)\n        \n        # Get labels\n        labels = self.df.iloc[actual_indices].label.values\n        \n        return (seq_features.astype('float16'), emoji_features.astype('float32')), labels\n\n    @property\n    def output_signature(self):\n        return (\n            (\n                tf.TensorSpec(shape=(None, 128, 768), dtype=tf.float16),\n                tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n            ),\n            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n        )\n\ndef train_model():\n    detector = DepressionDetector()\n    detector.load_data()\n    detector.extract_bert_features()\n    detector.build_model()\n\n    # Split data by index number, not by dataframe index values\n    indices = np.arange(len(detector.df))\n    train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n    train_indices, val_indices = train_test_split(train_indices, test_size=0.15, random_state=42)\n\n    # Create generators with proper output signatures, passing the full dataframe\n    train_gen = DataGenerator(train_indices, detector.df)\n    val_gen = DataGenerator(val_indices, detector.df)\n    test_gen = DataGenerator(test_indices, detector.df)\n\n    detector.model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=15,\n        callbacks=[\n            EarlyStopping(patience=5, restore_best_weights=True),\n            # Fixed file extension to .keras as required by newer versions of Keras\n            ModelCheckpoint('best_model.keras', save_best_only=True)\n        ]\n    )\n\n    # Evaluate\n    detector.model.evaluate(test_gen)\n    \n    # Predictions\n    all_preds = []\n    all_labels = []\n    \n    for i in range(len(test_gen)):\n        inputs, labels = test_gen[i]\n        predictions = detector.model.predict(inputs)\n        all_preds.append(predictions)\n        all_labels.append(labels)\n    \n    # Combine all batches\n    probs = np.concatenate(all_preds).flatten()\n    true_labels = np.concatenate(all_labels)\n    \n    print(f\"ROC AUC: {roc_auc_score(true_labels, probs):.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(true_labels, probs > 0.5))\n\nclass Predictor:\n    def __init__(self, model_path='best_model.keras'):  # Updated file extension here too\n        self.model = tf.keras.models.load_model(model_path)\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n        self.emoji_scores = pd.read_csv('/kaggle/input/emoji-sentiment/emoji_sentiment_score.csv').set_index('Char')['Sentiment score']\n\n    def predict(self, text):\n        # Preprocess text\n        clean_text = re.sub(r'http\\S+|@\\w+|#', '', text)\n\n        # Get emoji features\n        emojis = [c for c in clean_text if c in emoji.EMOJI_DATA]\n        emoji_score = np.mean([self.emoji_scores.get(e, 0.5) for e in emojis]) if emojis else 0.5\n\n        # Get BERT features\n        inputs = self.tokenizer(\n            emoji.demojize(clean_text),\n            return_tensors='tf', \n            padding='max_length',\n            truncation=True, \n            max_length=128\n        )\n        seq_output = self.bert_model(inputs).last_hidden_state[0].numpy().astype('float16')\n\n        # Make prediction\n        prediction = self.model.predict([seq_output[np.newaxis,...], np.array([[emoji_score]])])[0][0]\n\n        return {\n            'score': float(prediction),\n            'emoji_contribution': float(prediction - (0.7*prediction + 0.3*emoji_score)),\n            'text_analysis': clean_text,\n            'diagnosis': 'Depressed' if prediction > 0.5 else 'Not Depressed'\n        }\n\n# Execute training and prediction\nif __name__ == \"__main__\":\n    train_model()\n\n    # Example prediction\n    predictor = Predictor()\n    sample_text = \"I can't handle this pain anymore 😭 Everything feels meaningless...\"\n    result = predictor.predict(sample_text)\n    print(\"\\n=== Depression Analysis ===\")\n    print(f\"Text: {result['text_analysis']}\")\n    print(f\"Depression Score: {result['score']:.4f}\")\n    print(f\"Emoji Contribution: {result['emoji_contribution']:.4f}\")\n    print(f\"Diagnosis: {result['diagnosis']}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:59:03.304472Z","iopub.execute_input":"2025-04-20T16:59:03.305052Z","iopub.status.idle":"2025-04-20T17:17:12.899466Z","shell.execute_reply.started":"2025-04-20T16:59:03.305027Z","shell.execute_reply":"2025-04-20T17:17:12.898907Z"}},"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n100%|██████████| 1011/1011 [08:33<00:00,  1.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 128ms/step - accuracy: 0.7893 - loss: 0.4235 - val_accuracy: 0.9289 - val_loss: 0.1748\nEpoch 2/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 126ms/step - accuracy: 0.9368 - loss: 0.1619 - val_accuracy: 0.9304 - val_loss: 0.1684\nEpoch 3/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 126ms/step - accuracy: 0.9478 - loss: 0.1346 - val_accuracy: 0.9361 - val_loss: 0.1593\nEpoch 4/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 126ms/step - accuracy: 0.9577 - loss: 0.1119 - val_accuracy: 0.9356 - val_loss: 0.1527\nEpoch 5/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 125ms/step - accuracy: 0.9616 - loss: 0.1044 - val_accuracy: 0.9387 - val_loss: 0.1531\nEpoch 6/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 127ms/step - accuracy: 0.9647 - loss: 0.1010 - val_accuracy: 0.9371 - val_loss: 0.1684\nEpoch 7/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - accuracy: 0.9693 - loss: 0.0838 - val_accuracy: 0.9387 - val_loss: 0.1489\nEpoch 8/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 127ms/step - accuracy: 0.9769 - loss: 0.0720 - val_accuracy: 0.9387 - val_loss: 0.1576\nEpoch 9/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 127ms/step - accuracy: 0.9799 - loss: 0.0619 - val_accuracy: 0.9407 - val_loss: 0.1596\nEpoch 10/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 127ms/step - accuracy: 0.9837 - loss: 0.0504 - val_accuracy: 0.9397 - val_loss: 0.1655\nEpoch 11/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 127ms/step - accuracy: 0.9889 - loss: 0.0365 - val_accuracy: 0.9387 - val_loss: 0.1827\nEpoch 12/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 126ms/step - accuracy: 0.9909 - loss: 0.0350 - val_accuracy: 0.9361 - val_loss: 0.1873\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 106ms/step - accuracy: 0.9313 - loss: 0.1954\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step\nROC AUC: 0.9722\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.96      2601\n           1       0.86      0.81      0.84       632\n\n    accuracy                           0.94      3233\n   macro avg       0.91      0.89      0.90      3233\nweighted avg       0.94      0.94      0.94      3233\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step\n\n=== Depression Analysis ===\nText: I can't handle this pain anymore 😭 Everything feels meaningless...\nDepression Score: 0.3015\nEmoji Contribution: -0.0735\nDiagnosis: Not Depressed\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"predictor.predict(\"I am very suicidal 😭😭\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:30:46.277963Z","iopub.execute_input":"2025-04-20T17:30:46.278246Z","iopub.status.idle":"2025-04-20T17:30:46.527836Z","shell.execute_reply.started":"2025-04-20T17:30:46.278223Z","shell.execute_reply":"2025-04-20T17:30:46.527224Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'score': 0.092041015625,\n 'emoji_contribution': -0.13633769531249998,\n 'text_analysis': 'I am very suicidal 😭😭',\n 'diagnosis': 'Not Depressed'}"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"predictor.predict(\"I am very suicidal 😂😂\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:37:31.983076Z","iopub.execute_input":"2025-04-20T17:37:31.983365Z","iopub.status.idle":"2025-04-20T17:37:32.235516Z","shell.execute_reply.started":"2025-04-20T17:37:31.983345Z","shell.execute_reply":"2025-04-20T17:37:32.234964Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'score': 0.0250396728515625,\n 'emoji_contribution': -0.10933809814453122,\n 'text_analysis': 'I am very suicidal 😂😂',\n 'diagnosis': 'Not Depressed'}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"predictor.predict(\"😭😭\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:02:08.363258Z","iopub.execute_input":"2025-04-20T19:02:08.363777Z","iopub.status.idle":"2025-04-20T19:02:08.626702Z","shell.execute_reply.started":"2025-04-20T19:02:08.363751Z","shell.execute_reply":"2025-04-20T19:02:08.626015Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'score': 0.003662109375,\n 'emoji_contribution': -0.16285136718749998,\n 'text_analysis': '😭😭',\n 'diagnosis': 'Not Depressed'}"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport emoji\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, Input, Concatenate, Attention\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import BertTokenizer, TFBertModel\nimport gc\nfrom tqdm import tqdm\n\n# Configuration\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nclass DepressionDetector:\n    def __init__(self):\n        self.emoji_to_score = None\n        self.model = None\n        self.tokenizer = None\n        self.max_length = 128\n\n    def load_data(self):\n        emoji_scores = pd.read_csv('/kaggle/input/emoji-sentiment/emoji_sentiment_score.csv')\n        self.emoji_to_score = dict(zip(emoji_scores['Char'], emoji_scores['Sentiment score']))\n\n        # Load and balance datasets\n        dep = pd.read_csv('/kaggle/input/twitter-depression-dataset/clean_d_tweets.csv', usecols=['tweet']).assign(label=1)\n        non_dep = pd.read_csv('/kaggle/input/twitter-depression-dataset/clean_non_d_tweets.csv', usecols=['tweet']).assign(label=0)\n        non_dep = non_dep.sample(n=len(dep), random_state=42)\n\n        # Load sarcasm data\n        sarcasm = pd.concat([chunk[chunk.label == 1] for chunk in\n                           pd.read_csv('/kaggle/input/sarcasm/train-balanced-sarcasm.csv', chunksize=10000)])\n        sarcasm = sarcasm.sample(10000).assign(label=0)\n\n        self.df = pd.concat([\n            dep.rename(columns={'tweet': 'text'}),\n            non_dep.rename(columns={'tweet': 'text'}),\n            sarcasm.rename(columns={'comment': 'text'})\n        ]).sample(frac=1).reset_index(drop=True)  # Reset index for consistency\n\n        # Preprocess\n        self.df['processed'] = self.df.text.apply(self.clean_text)\n        self.df['emoji_score'] = self.df.processed.apply(self.get_emoji_score)\n        self.df['bert_text'] = self.df.processed.apply(emoji.demojize)\n\n    def clean_text(self, text):\n        text = re.sub(r'http\\S+|@\\w+|#', '', str(text))\n        return re.sub(r'\\s+', ' ', text).strip()\n\n    def get_emoji_score(self, text):\n        emojis = [c for c in text if c in emoji.EMOJI_DATA]\n        if not emojis: return 0.5\n        return np.mean([self.emoji_to_score.get(e, 0.5) for e in emojis])\n\n    def extract_bert_features(self):\n        os.makedirs('bert_features', exist_ok=True)\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n\n        for i in tqdm(range(0, len(self.df), 16)):\n            batch = self.df.bert_text.iloc[i:i+16].tolist()\n            inputs = self.tokenizer(batch, return_tensors='tf', padding='max_length',\n                                   truncation=True, max_length=self.max_length)\n            outputs = bert_model(inputs)\n\n            for j, idx in enumerate(range(i, min(i+16, len(self.df)))):  # Use actual indices\n                np.save(f\"bert_features/pooled_{idx}.npy\", outputs.pooler_output[j].numpy())\n                np.savez_compressed(f\"bert_features/sequence_{idx}.npy\", outputs.last_hidden_state[j].numpy())\n\n        del bert_model\n        gc.collect()\n\n    def build_model(self):\n        # Text processing branch\n        sequence_input = Input(shape=(self.max_length, 768), dtype=tf.float16)\n        lstm = Bidirectional(LSTM(128, return_sequences=True))(sequence_input)\n        attention = Attention()([lstm, lstm])\n        text_features = Dense(128)(attention[:, -1, :])\n\n        # Emoji processing branch\n        emoji_input = Input(shape=(1,), dtype=tf.float32)\n        emoji_features = Dense(64, activation='relu')(emoji_input)\n        emoji_features = Dense(32, activation='relu')(emoji_features)\n\n        # Combined features\n        combined = Concatenate()([text_features, emoji_features])\n        combined = Dense(128, activation='relu')(combined)\n        output = Dense(1, activation='sigmoid')(combined)\n\n        self.model = Model(inputs=[sequence_input, emoji_input], outputs=output)\n        self.model.compile(\n            optimizer=tf.keras.mixed_precision.LossScaleOptimizer(Adam(3e-5)),\n            loss='binary_crossentropy',\n            metrics=['accuracy']\n        )\n        \n        # Save the model architecture for later recreation\n        with open('model_architecture.json', 'w') as f:\n            f.write(self.model.to_json())\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, indices, df, batch_size=32):\n        \"\"\"\n        Modified to receive the entire dataframe instead of separate components\n        \"\"\"\n        self.indices = indices\n        self.df = df\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return int(np.ceil(len(self.indices) / self.batch_size))\n\n    def __getitem__(self, idx):\n        # Calculate start and end indices for this batch\n        start_idx = idx * self.batch_size\n        end_idx = min((idx + 1) * self.batch_size, len(self.indices))\n        \n        # Get actual indices for this batch\n        actual_indices = self.indices[start_idx:end_idx]\n        \n        # Load BERT sequence features\n        seq_features = np.stack([np.load(f\"bert_features/sequence_{i}.npy.npz\")['arr_0'] for i in actual_indices])\n        \n        # Get emoji features directly from dataframe using the same indices\n        emoji_features = self.df.iloc[actual_indices].emoji_score.values.reshape(-1, 1)\n        \n        # Get labels\n        labels = self.df.iloc[actual_indices].label.values\n        \n        return (seq_features.astype('float16'), emoji_features.astype('float32')), labels\n\n    @property\n    def output_signature(self):\n        return (\n            (\n                tf.TensorSpec(shape=(None, 128, 768), dtype=tf.float16),\n                tf.TensorSpec(shape=(None, 1), dtype=tf.float32)\n            ),\n            tf.TensorSpec(shape=(None,), dtype=tf.float32)\n        )\n\ndef train_model():\n    detector = DepressionDetector()\n    detector.load_data()\n    detector.extract_bert_features()\n    detector.build_model()\n\n    # Split data by index number, not by dataframe index values\n    indices = np.arange(len(detector.df))\n    train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n    train_indices, val_indices = train_test_split(train_indices, test_size=0.15, random_state=42)\n\n    # Create generators with proper output signatures, passing the full dataframe\n    train_gen = DataGenerator(train_indices, detector.df)\n    val_gen = DataGenerator(val_indices, detector.df)\n    test_gen = DataGenerator(test_indices, detector.df)\n\n    # Fix: Changed ModelCheckpoint filepath to end with .weights.h5\n    detector.model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=15,\n        callbacks=[\n            EarlyStopping(patience=5, restore_best_weights=True),\n            ModelCheckpoint('best_model.weights.h5', save_best_only=True, save_weights_only=True)\n        ]\n    )\n\n    # Save final weights - also fixing this filename\n    detector.model.save_weights('final_model.weights.h5')\n\n    # Evaluate\n    detector.model.evaluate(test_gen)\n    \n    # Predictions\n    all_preds = []\n    all_labels = []\n    \n    for i in range(len(test_gen)):\n        inputs, labels = test_gen[i]\n        predictions = detector.model.predict(inputs)\n        all_preds.append(predictions)\n        all_labels.append(labels)\n    \n    # Combine all batches\n    probs = np.concatenate(all_preds).flatten()\n    true_labels = np.concatenate(all_labels)\n    \n    print(f\"ROC AUC: {roc_auc_score(true_labels, probs):.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(true_labels, probs > 0.5))\n    \n    # Save metadata required for prediction\n    metadata = {\n        'max_length': detector.max_length,\n        'emoji_to_score': detector.emoji_to_score\n    }\n    import pickle\n    with open('model_metadata.pkl', 'wb') as f:\n        pickle.dump(metadata, f)\n\nclass Predictor:\n    def __init__(self, weights_path='best_model.weights.h5', metadata_path='model_metadata.pkl', architecture_path='model_architecture.json'):\n        # Load metadata\n        import pickle\n        with open(metadata_path, 'rb') as f:\n            metadata = pickle.load(f)\n        \n        self.max_length = metadata['max_length']\n        \n        # Initialize tokenizer and BERT model\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n        \n        # Load emoji scores\n        self.emoji_scores = pd.read_csv('emoji_sentiment_score.csv').set_index('Char')['Sentiment score']\n        \n        # Recreate model architecture\n        with open(architecture_path, 'r') as f:\n            model_json = f.read()\n        \n        self.model = tf.keras.models.model_from_json(model_json)\n        \n        # Load weights\n        self.model.load_weights(weights_path)\n        \n        # Compile model\n        self.model.compile(\n            optimizer=tf.keras.mixed_precision.LossScaleOptimizer(Adam(3e-5)),\n            loss='binary_crossentropy', \n            metrics=['accuracy']\n        )\n\n    def predict(self, text):\n        # Preprocess text\n        clean_text = re.sub(r'http\\S+|@\\w+|#', '', text)\n\n        # Get emoji features and analyze emojis\n        emojis = [c for c in clean_text if c in emoji.EMOJI_DATA]\n        emoji_score = np.mean([self.emoji_scores.get(e, 0.5) for e in emojis]) if emojis else 0.5\n        \n        # Get BERT features for text analysis (without emojis)\n        demojized_text = emoji.demojize(clean_text)\n        inputs = self.tokenizer(\n            demojized_text,\n            return_tensors='tf',\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length\n        )\n        seq_output = self.bert_model(inputs).last_hidden_state[0].numpy().astype('float16')\n\n        # Make base prediction from text content\n        base_prediction = self.model.predict([seq_output[np.newaxis,...], np.array([[0.5]])])[0][0]\n        \n        # Make prediction with emoji influence\n        full_prediction = self.model.predict([seq_output[np.newaxis,...], np.array([[emoji_score]])])[0][0]\n        \n        # Calculate emoji influence (0-1 scale)\n        # For sad emojis (score > 0.5), emoji_influence will be higher\n        # For happy emojis (score < 0.5), emoji_influence will be lower\n        emoji_influence = emoji_score if emojis else 0.5\n        \n        # Calculate final adjusted score with proper emoji influence\n        # Base on text content but adjust based on emoji sentiment\n        adjusted_score = base_prediction\n        if emojis:\n            # If sad emojis (>0.5), increase depression score\n            if emoji_score > 0.5:\n                boost_factor = (emoji_score - 0.5) * 2  # Convert 0.5-1.0 range to 0.0-1.0\n                adjusted_score = min(1.0, base_prediction + (1.0 - base_prediction) * boost_factor * 0.5)\n            # If happy emojis (<0.5), decrease depression score\n            else:\n                reduction_factor = (0.5 - emoji_score) * 2  # Convert 0.0-0.5 range to 0.0-1.0\n                adjusted_score = max(0.0, base_prediction - base_prediction * reduction_factor * 0.5)\n\n        return {\n            'base_score': float(base_prediction),  # Score without emoji influence\n            'emoji_score': float(emoji_score),     # Raw emoji sentiment score (0-1)\n            'adjusted_score': float(adjusted_score),  # Final score with emoji adjustment\n            'emoji_influence': float(emoji_influence),  # How much emojis affected the score (0-1)\n            'text_analysis': clean_text,\n            'diagnosis': 'Depressed' if adjusted_score > 0.5 else 'Not Depressed'\n        }\n\n# Execute training and prediction\nif __name__ == \"__main__\":\n    try:\n        train_model()\n        \n        # Wait a moment to ensure files are properly saved\n        import time\n        time.sleep(1)\n        \n        # Example prediction\n        predictor = Predictor()\n        \n        # Test with different emoji variations\n        texts = [\n            \"I am very suicidal\",\n            \"I am very suicidal 😭😭\",\n            \"I am very suicidal 😂😂\",\n            \"I feel so empty inside 😔\",\n            \"Everything is going great! 🎉\",\n            \"I can't handle this pain anymore\"\n        ]\n        \n        print(\"\\n=== Depression Analysis Results ===\")\n        for test_text in texts:\n            result = predictor.predict(test_text)\n            print(f\"\\nText: {result['text_analysis']}\")\n            print(f\"Base Score (text only): {result['base_score']:.4f}\")\n            print(f\"Emoji Score: {result['emoji_score']:.4f}\")\n            print(f\"Emoji Influence: {result['emoji_influence']:.4f}\")\n            print(f\"Adjusted Score: {result['adjusted_score']:.4f}\")\n            print(f\"Diagnosis: {result['diagnosis']}\")\n        \n    except Exception as e:\n        print(f\"Error encountered: {str(e)}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"BHAIYA iss code hath mat laganaa yeh cahl rha hai (BELOWWWW)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport emoji\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, Bidirectional, Input, Concatenate, Attention\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\nimport matplotlib.pyplot as plt\nfrom transformers import BertTokenizer, TFBertModel\nimport gc\nimport pickle\nfrom tqdm import tqdm\n\n# Enable mixed precision\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nclass DepressionDetector:\n    def __init__(self):\n        self.emoji_to_score = None\n        self.model = None\n        self.tokenizer = None\n        self.max_length = 128\n\n    def load_data(self):\n        emoji_scores = pd.read_csv('/kaggle/input/emoji-sentiment/emoji_sentiment_score.csv')\n        self.emoji_to_score = dict(zip(emoji_scores['Char'], emoji_scores['Sentiment score']))\n        dep = pd.read_csv('/kaggle/input/twitter-depression-dataset/clean_d_tweets.csv', usecols=['tweet']).assign(label=1)\n        non_dep = pd.read_csv('/kaggle/input/twitter-depression-dataset/clean_non_d_tweets.csv', usecols=['tweet']).assign(label=0)\n        non_dep = non_dep.sample(n=len(dep), random_state=42)\n        sarcasm = pd.concat([chunk[chunk.label == 1] for chunk in pd.read_csv('/kaggle/input/sarcasm/train-balanced-sarcasm.csv', chunksize=10000)])\n        sarcasm = sarcasm.sample(10000).assign(label=0)\n        self.df = pd.concat([\n            dep.rename(columns={'tweet': 'text'}),\n            non_dep.rename(columns={'tweet': 'text'}),\n            sarcasm.rename(columns={'comment': 'text'})\n        ]).sample(frac=1).reset_index(drop=True)\n        self.df['processed'] = self.df.text.apply(self.clean_text)\n        self.df['emoji_score'] = self.df.processed.apply(self.get_emoji_score)\n        self.df['bert_text'] = self.df.processed.apply(emoji.demojize)\n\n    def clean_text(self, text):\n        text = re.sub(r'http\\S+|@\\w+|#', '', str(text))\n        return re.sub(r'\\s+', ' ', text).strip()\n\n    def get_emoji_score(self, text):\n        emojis = [c for c in text if c in emoji.EMOJI_DATA]\n        return np.mean([self.emoji_to_score.get(e, 0.5) for e in emojis]) if emojis else 0.5\n\n    def extract_bert_features(self):\n        os.makedirs('bert_features', exist_ok=True)\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n        for i in tqdm(range(0, len(self.df), 16)):\n            batch = self.df.bert_text.iloc[i:i+16].tolist()\n            inputs = self.tokenizer(batch, return_tensors='tf', padding='max_length', truncation=True, max_length=self.max_length)\n            outputs = bert_model(inputs)\n            for j, idx in enumerate(range(i, min(i+16, len(self.df)))):\n                np.save(f\"bert_features/pooled_{idx}.npy\", outputs.pooler_output[j].numpy())\n                np.savez_compressed(f\"bert_features/sequence_{idx}.npy\", outputs.last_hidden_state[j].numpy())\n        del bert_model\n        gc.collect()\n\n    def build_model(self):\n        sequence_input = Input(shape=(self.max_length, 768), dtype=tf.float16)\n        lstm = Bidirectional(LSTM(128, return_sequences=True))(sequence_input)\n        attention = Attention()([lstm, lstm])\n        text_features = Dense(128)(attention[:, -1, :])\n        emoji_input = Input(shape=(1,), dtype=tf.float32)\n        emoji_features = Dense(64, activation='relu')(emoji_input)\n        emoji_features = Dense(32, activation='relu')(emoji_features)\n        combined = Concatenate()([text_features, emoji_features])\n        combined = Dense(128, activation='relu')(combined)\n        output = Dense(1, activation='sigmoid')(combined)\n        self.model = Model(inputs=[sequence_input, emoji_input], outputs=output)\n        self.model.compile(optimizer=tf.keras.mixed_precision.LossScaleOptimizer(Adam(3e-5)), loss='binary_crossentropy', metrics=['accuracy'])\n        with open('model_architecture.json', 'w') as f:\n            f.write(self.model.to_json())\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, indices, df, batch_size=32):\n        self.indices = indices\n        self.df = df\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return int(np.ceil(len(self.indices) / self.batch_size))\n\n    def __getitem__(self, idx):\n        start_idx = idx * self.batch_size\n        end_idx = min((idx + 1) * self.batch_size, len(self.indices))\n        actual_indices = self.indices[start_idx:end_idx]\n        seq_features = np.stack([np.load(f\"bert_features/sequence_{i}.npy.npz\")['arr_0'] for i in actual_indices])\n        emoji_features = self.df.iloc[actual_indices].emoji_score.values.reshape(-1, 1)\n        labels = self.df.iloc[actual_indices].label.values\n        return (seq_features.astype('float16'), emoji_features.astype('float32')), labels\n\ndef train_model():\n    detector = DepressionDetector()\n    detector.load_data()\n    detector.extract_bert_features()\n    detector.build_model()\n    indices = np.arange(len(detector.df))\n    train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n    train_indices, val_indices = train_test_split(train_indices, test_size=0.15, random_state=42)\n    train_gen = DataGenerator(train_indices, detector.df)\n    val_gen = DataGenerator(val_indices, detector.df)\n    test_gen = DataGenerator(test_indices, detector.df)\n    detector.model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=15,\n        callbacks=[\n            EarlyStopping(patience=5, restore_best_weights=True),\n            ModelCheckpoint('best_model.weights.h5', save_best_only=True, save_weights_only=True)\n        ]\n    )\n    detector.model.save_weights('final_model.weights.h5')\n    detector.model.evaluate(test_gen)\n    all_preds, all_labels = [], []\n    for i in range(len(test_gen)):\n        inputs, labels = test_gen[i]\n        predictions = detector.model.predict(inputs)\n        all_preds.append(predictions)\n        all_labels.append(labels)\n    probs = np.concatenate(all_preds).flatten()\n    true_labels = np.concatenate(all_labels)\n    print(f\"ROC AUC: {roc_auc_score(true_labels, probs):.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(true_labels, probs > 0.5))\n    metadata = {'max_length': detector.max_length, 'emoji_to_score': detector.emoji_to_score}\n    with open('model_metadata.pkl', 'wb') as f:\n        pickle.dump(metadata, f)\n\nclass Predictor:\n    def __init__(self, weights_path='best_model.weights.h5', metadata_path='model_metadata.pkl', architecture_path='model_architecture.json'):\n        with open(metadata_path, 'rb') as f:\n            metadata = pickle.load(f)\n        self.max_length = metadata['max_length']\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n        self.emoji_scores = pd.read_csv('/kaggle/input/emoji-sentiment/emoji_sentiment_score.csv').set_index('Char')['Sentiment score']\n        with open(architecture_path, 'r') as f:\n            model_json = f.read()\n        self.model = tf.keras.models.model_from_json(model_json)\n        self.model.load_weights(weights_path)\n        self.model.compile(optimizer=tf.keras.mixed_precision.LossScaleOptimizer(Adam(3e-5)), loss='binary_crossentropy', metrics=['accuracy'])\n\n    def predict(self, text):\n        clean_text = re.sub(r'http\\S+|@\\w+|#', '', text)\n        emojis = [c for c in clean_text if c in emoji.EMOJI_DATA]\n        text_only = ''.join(c for c in clean_text if c not in emoji.EMOJI_DATA).strip()\n        emoji_score = np.mean([self.emoji_scores.get(e, 0.5) for e in emojis]) if emojis else 0.5\n        demojized_text = emoji.demojize(text_only)\n        \n        inputs = self.tokenizer(demojized_text, return_tensors='tf', padding='max_length', truncation=True, max_length=self.max_length)\n        seq_output = self.bert_model(inputs).last_hidden_state[0].numpy().astype('float16')\n        base_score = self.model.predict([seq_output[np.newaxis,...], np.array([[0.5]])])[0][0]\n        \n        # Combine base_score with emoji sentiment influence\n        final_score = (0.85 * base_score) + (0.15 * (1 - emoji_score))\n        \n        return {\n            'original_text': text,\n            'text_without_emojis': text_only,\n            'emojis_found': emojis,\n            'base_score': float(base_score),\n            'emoji_score': float(emoji_score),\n            'final_score': float(final_score),\n            'diagnosis': 'Depressed' if final_score > 0.5 else 'Not Depressed'\n        }\n\n            \n\n# Train and predict\ntry:\n    train_model()\n    import time; time.sleep(1)\n    predictor = Predictor()\n    texts = [\n        \"I am very suicidal\",\n        \"I am very suicidal 😭😭\",\n        \"I am very suicidal 😂😂\",\n        \"I feel so empty inside 😔\",\n        \"Everything is going great! 🎉\",\n        \"I can't handle this pain anymore\"\n    ]\n    print(\"\\n=== Depression Analysis Results ===\")\n    for text in texts:\n        result = predictor.predict(text)\n        \n        print(f\"Base Score (text only): {result['base_score']:.4f}\")\n        print(f\"Emoji Score: {result['emoji_score']:.4f}\")\n        print(f\"Emoji Influence: {result['emoji_influence']:.4f}\")\n        print(f\"Adjusted Score: {result['adjusted_score']:.4f}\")\n        print(f\"Diagnosis: {result['diagnosis']}\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    import traceback; traceback.print_exc()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:46:06.730063Z","iopub.execute_input":"2025-04-20T19:46:06.731000Z","iopub.status.idle":"2025-04-20T20:03:33.482459Z","shell.execute_reply.started":"2025-04-20T19:46:06.730975Z","shell.execute_reply":"2025-04-20T20:03:33.481525Z"}},"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n100%|██████████| 1011/1011 [08:35<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 129ms/step - accuracy: 0.7948 - loss: 0.4150 - val_accuracy: 0.9325 - val_loss: 0.1766\nEpoch 2/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 127ms/step - accuracy: 0.9334 - loss: 0.1628 - val_accuracy: 0.9366 - val_loss: 0.1602\nEpoch 3/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 126ms/step - accuracy: 0.9477 - loss: 0.1287 - val_accuracy: 0.9376 - val_loss: 0.1586\nEpoch 4/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 125ms/step - accuracy: 0.9543 - loss: 0.1125 - val_accuracy: 0.9412 - val_loss: 0.1486\nEpoch 5/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 125ms/step - accuracy: 0.9611 - loss: 0.1012 - val_accuracy: 0.9407 - val_loss: 0.1569\nEpoch 6/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 127ms/step - accuracy: 0.9689 - loss: 0.0858 - val_accuracy: 0.9443 - val_loss: 0.1426\nEpoch 7/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 126ms/step - accuracy: 0.9703 - loss: 0.0822 - val_accuracy: 0.9418 - val_loss: 0.1543\nEpoch 8/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 127ms/step - accuracy: 0.9790 - loss: 0.0650 - val_accuracy: 0.9397 - val_loss: 0.1560\nEpoch 9/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 126ms/step - accuracy: 0.9806 - loss: 0.0575 - val_accuracy: 0.9418 - val_loss: 0.1696\nEpoch 10/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 126ms/step - accuracy: 0.9833 - loss: 0.0489 - val_accuracy: 0.9376 - val_loss: 0.1888\nEpoch 11/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 125ms/step - accuracy: 0.9861 - loss: 0.0401 - val_accuracy: 0.9392 - val_loss: 0.2031\n\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 106ms/step - accuracy: 0.9383 - loss: 0.1669\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step\nROC AUC: 0.9761\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.96      2601\n           1       0.84      0.86      0.85       632\n\n    accuracy                           0.94      3233\n   macro avg       0.90      0.91      0.91      3233\nweighted avg       0.94      0.94      0.94      3233\n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"\n=== Depression Analysis Results ===\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791ms/step\nError: 'text_analysis'\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/69562147.py\", line 205, in <cell line: 0>\n    print(f\"\\nText: {result['text_analysis']}\")\n                     ~~~~~~^^^^^^^^^^^^^^^^^\nKeyError: 'text_analysis'\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\nfor text in texts:\n        result = predictor.predict(text)\n        final_score = (0.85 * result['base_score']) + (0.15 * result['emoji_score'])\n        print(text)\n        print(f\"Base Score (text only): {result['base_score']:.4f}\")\n        print(f\"Emoji Score: {result['emoji_score']:.4f}\")\n    \n        print(final_score)\n        \n        print(f\"Diagnosis: {result['diagnosis']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:48:18.602148Z","iopub.execute_input":"2025-04-20T20:48:18.602683Z","iopub.status.idle":"2025-04-20T20:48:20.130031Z","shell.execute_reply.started":"2025-04-20T20:48:18.602660Z","shell.execute_reply":"2025-04-20T20:48:20.129303Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\nI am very suicidal\nBase Score (text only): 0.9932\nEmoji Score: 0.5000\n0.9191894531249999\nDiagnosis: Depressed\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nI am very suicidal 😭😭\nBase Score (text only): 0.9932\nEmoji Score: 0.5465\n0.926164453125\nDiagnosis: Depressed\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\nI am very suicidal 😂😂\nBase Score (text only): 0.9932\nEmoji Score: 0.3895\n0.9026144531249999\nDiagnosis: Depressed\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nI feel so empty inside 😔\nBase Score (text only): 0.9971\nEmoji Score: 0.5730\n0.933459765625\nDiagnosis: Depressed\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nEverything is going great! 🎉\nBase Score (text only): 0.0005\nEmoji Score: 0.1310\n0.02010719146728516\nDiagnosis: Not Depressed\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nI can't handle this pain anymore\nBase Score (text only): 0.9980\nEmoji Score: 0.5000\n0.9233398437499999\nDiagnosis: Depressed\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"text = input('Enter text')\nresult = predictor.predict(text)\nfinal_score = (0.85 * result['base_score']) + (0.15 * result['emoji_score'])\nprint(text)\nprint(f\"Base Score (text only): {result['base_score']:.4f}\")\nprint(f\"Emoji Score: {result['emoji_score']:.4f}\")\n\nprint(final_score)\n\nprint(f\"Diagnosis: {result['diagnosis']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:51:17.773974Z","iopub.execute_input":"2025-04-20T20:51:17.774445Z","iopub.status.idle":"2025-04-20T20:51:32.184548Z","shell.execute_reply.started":"2025-04-20T20:51:17.774422Z","shell.execute_reply":"2025-04-20T20:51:32.183810Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter text 😂😂\n"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n😂😂\nBase Score (text only): 0.0149\nEmoji Score: 0.3895\n0.07109017639160155\nDiagnosis: Not Depressed\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"UPARRRR WALE CODE KO HAATH MAT LAGANAAA","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport pickle\nimport emoji\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, TFBertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional, Attention, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.utils import Sequence\n\n# Enable mixed precision\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n\nclass DepressionDetector:\n    def _init_(self):\n        self.emoji_to_score = {}\n        self.model = None\n        self.tokenizer = None\n        self.max_length = 128\n        self.df = None\n\n    def load_data(self):\n        # Load emoji sentiment scores\n        emoji_scores = pd.read_csv('/kaggle/input/emoji-sentiment/emoji_sentiment_score.csv')\n        self.emoji_to_score = dict(zip(emoji_scores['Char'], emoji_scores['Sentiment score']))\n        # Load tweets\n        dep = pd.read_csv('/kaggle/input/twitter-depression-dataset/clean_d_tweets.csv', usecols=['tweet']).assign(label=1)\n        non_dep = pd.read_csv('/kaggle/input/twitter-depression-dataset/clean_non_d_tweets.csv', usecols=['tweet']).assign(label=0)\n        non_dep = non_dep.sample(n=len(dep), random_state=42)\n        sarcasm = pd.concat(\n            [chunk[chunk.label == 1] \n             for chunk in pd.read_csv('/kaggle/input/sarcasm/train-balanced-sarcasm.csv', chunksize=10000)]\n        ).sample(10000).assign(label=0)\n        # Combine\n        self.df = pd.concat([\n            dep.rename(columns={'tweet': 'text'}),\n            non_dep.rename(columns={'tweet': 'text'}),\n            sarcasm.rename(columns={'comment': 'text'})\n        ]).sample(frac=1, random_state=42).reset_index(drop=True)\n        # Preprocess\n        self.df['processed'] = self.df.text.apply(self.clean_text)\n        self.df['emoji_score'] = self.df.processed.apply(self.get_emoji_score)\n        self.df['bert_text'] = self.df.processed.apply(emoji.demojize)\n\n    def clean_text(self, text):\n        text = re.sub(r'http\\S+|@\\w+|#', '', str(text))\n        return re.sub(r'\\s+', ' ', text).strip()\n\n    def get_emoji_score(self, text):\n        emojis = [c for c in text if c in emoji.EMOJI_DATA]\n        if not emojis:\n            return 0.5\n        return np.mean([self.emoji_to_score.get(e, 0.5) for e in emojis])\n\n    def extract_bert_features(self):\n        os.makedirs('bert_features', exist_ok=True)\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n        for i in tqdm(range(0, len(self.df), 16)):\n            batch = self.df.bert_text.iloc[i:i+16].tolist()\n            inputs = self.tokenizer(batch, return_tensors='tf',\n                                    padding='max_length', truncation=True,\n                                    max_length=self.max_length)\n            outputs = bert_model(**inputs)\n            for j, idx in enumerate(range(i, min(i+16, len(self.df)))):\n                # Save sequence features only\n                np.savez_compressed(f\"bert_features/sequence_{idx}.npz\",\n                                    arr=outputs.last_hidden_state[j].numpy())\n        del bert_model\n        gc.collect()\n\n    def build_model(self):\n        # Text sequence input\n        sequence_input = Input(shape=(self.max_length, 768), dtype=tf.float16, name='text_input')\n        x = Bidirectional(LSTM(128, return_sequences=True))(sequence_input)\n        x = Attention()([x, x])\n        text_features = Dense(128, activation='relu')(x[:, -1, :])\n        # Emoji score input\n        emoji_input = Input(shape=(1,), dtype=tf.float32, name='emoji_input')\n        e = Dense(64, activation='relu')(emoji_input)\n        e = Dense(32, activation='relu')(e)\n        # Combine\n        combined = Concatenate()([text_features, e])\n        combined = Dense(128, activation='relu')(combined)\n        output = Dense(1, activation='sigmoid', name='output')(combined)\n        self.model = Model(inputs=[sequence_input, emoji_input], outputs=output)\n        optimizer = Adam(3e-5)\n        self.model.compile(optimizer=optimizer,\n                           loss='binary_crossentropy',\n                           metrics=['accuracy'])\n        # Save architecture\n        with open('model_architecture.json', 'w') as f:\n            f.write(self.model.to_json())\n\nclass DataGenerator(Sequence):\n    def _init_(self, indices, df, batch_size=32):\n        self.indices = indices\n        self.df = df\n        self.batch_size = batch_size\n\n    def _len_(self):\n        return int(np.ceil(len(self.indices) / self.batch_size))\n\n    def _getitem_(self, idx):\n        start = idx * self.batch_size\n        end = min((idx + 1) * self.batch_size, len(self.indices))\n        batch_idx = self.indices[start:end]\n        # Load BERT sequence features\n        seqs = np.stack([\n            np.load(f\"bert_features/sequence_{i}.npz\")['arr']\n            for i in batch_idx\n        ])\n        emojis = self.df.loc[batch_idx, 'emoji_score'].values.reshape(-1, 1)\n        labels = self.df.loc[batch_idx, 'label'].values\n        return [seqs.astype('float16'), emojis.astype('float32')], labels\n\nclass Predictor:\n    def __init__(self,\n                 weights_path='best_model.weights.h5',\n                 metadata_path='model_metadata.pkl',\n                 architecture_path='model_architecture.json'):\n        # Load metadata\n        metadata = pickle.load(open(metadata_path, 'rb'))\n        self.max_length = metadata['max_length']\n        self.emoji_to_score = metadata['emoji_to_score']\n        # Tokenizer & BERT\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n        # Load model\n        model_json = open(architecture_path, 'r').read()\n        self.model = tf.keras.models.model_from_json(model_json)\n        self.model.load_weights(weights_path)\n        self.model.compile(optimizer=Adam(3e-5),\n                           loss='binary_crossentropy',\n                           metrics=['accuracy'])\n\n    def predict(self, text):\n        # Clean & extract emojis\n        clean_text = re.sub(r'http\\S+|@\\w+|#', '', text)\n        emojis = [c for c in clean_text if c in emoji.EMOJI_DATA]\n        text_only = ''.join(c for c in clean_text if c not in emoji.EMOJI_DATA).strip()\n        emoji_score = np.mean([self.emoji_to_score.get(e, 0.5) for e in emojis]) if emojis else 0.5\n        demojized = emoji.demojize(text_only)\n        # BERT inputs\n        inputs = self.tokenizer(demojized, return_tensors='tf',\n                                padding='max_length', truncation=True,\n                                max_length=self.max_length)\n        seq_output = self.bert_model(**inputs).last_hidden_state[0].numpy().astype('float16')\n        base_score = float(self.model.predict(\n            [seq_output[np.newaxis, ...], np.array([[emoji_score]])]\n        )[0][0])\n        # Emoji influence & final\n        emoji_influence = 0.15 * (1 - emoji_score)\n        adjusted_score = 0.85 * base_score + emoji_influence\n        diagnosis = 'Depressed' if adjusted_score > 0.5 else 'Not Depressed'\n        return {\n            'original_text': text,\n            'base_score': base_score,\n            'emoji_score': emoji_score,\n            'emoji_influence': emoji_influence,\n            'adjusted_score': adjusted_score,\n            'diagnosis': diagnosis\n        }\n\ndef train_model():\n    detector = DepressionDetector()\n    detector.load_data()\n    detector.extract_bert_features()\n    detector.build_model()\n\n    # Split data\n    idx = np.arange(len(detector.df))\n    train_idx, test_idx = train_test_split(idx, test_size=0.2, random_state=42)\n    train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=42)\n\n    train_gen = DataGenerator(train_idx, detector.df)\n    val_gen = DataGenerator(val_idx, detector.df)\n    test_gen = DataGenerator(test_idx, detector.df)\n\n    # Train\n    detector.model.fit(\n        train_gen,\n        validation_data=val_gen,\n        epochs=15,\n        callbacks=[\n            EarlyStopping(patience=5, restore_best_weights=True),\n            ModelCheckpoint('best_model.weights.h5', save_best_only=True, save_weights_only=True)\n        ]\n    )\n    # Save final weights and metadata\n    detector.model.save_weights('final_model.weights.h5')\n    metadata = {'max_length': detector.max_length, 'emoji_to_score': detector.emoji_to_score}\n    with open('model_metadata.pkl', 'wb') as f:\n        pickle.dump(metadata, f)\n\n    # Evaluate\n    print(\"\\n=== Test Evaluation ===\")\n    detector.model.evaluate(test_gen)\n    all_preds, all_labels = [], []\n    for i in range(len(test_gen)):\n        (x_seq, x_emoji), y = test_gen[i]\n        p = detector.model.predict([x_seq, x_emoji]).flatten()\n        all_preds.append(p)\n        all_labels.append(y)\n    probs = np.concatenate(all_preds)\n    true = np.concatenate(all_labels)\n    print(f\"ROC AUC: {roc_auc_score(true, probs):.4f}\")\n    print(classification_report(true, probs > 0.5))\n\nif __name__ == \"__main__\":\n    # Train and save everything\n    train_model()\n\n    # Initialize predictor\n    predictor = Predictor(weights_path='best_model.weights.h5',\n                          metadata_path='model_metadata.pkl',\n                          architecture_path='model_architecture.json')\n\n    # Demo texts\n    demo_texts = [\n        \"I am very suicidal\",\n        \"I am very suicidal 😭😭\",\n        \"I am very suicidal 😂😂\",\n        \"I feel so empty inside 😔\",\n        \"Everything is going great! 🎉\",\n        \"I can't handle this pain anymore\"\n    ]\n    print(\"\\n=== Demo Analysis Results ===\")\n    for txt in demo_texts:\n        res = predictor.predict(txt)\n        print(f\"\\nText: {res['original_text']}\")\n        print(f\"  Base Score: {res['base_score']:.4f}\")\n        print(f\"  Emoji Score: {res['emoji_score']:.4f}\")\n        print(f\"  Emoji Influence: {res['emoji_influence']:.4f}\")\n        print(f\"  Adjusted Score: {res['adjusted_score']:.4f}\")\n        print(f\"  Diagnosis: {res['diagnosis']}\")\n\n    # Custom input\n    while True:\n        custom = input(\"\\nEnter custom text (or 'exit' to quit): \")\n        if custom.lower() == 'exit':\n            break\n        res = predictor.predict(custom)\n        print(f\"\\nText: {res['original_text']}\")\n        print(f\"  Base Score: {res['base_score']:.4f}\")\n        print(f\"  Emoji Score: {res['emoji_score']:.4f}\")\n        print(f\"  Emoji Influence: {res['emoji_influence']:.4f}\")\n        print(f\"  Adjusted Score: {res['adjusted_score']:.4f}\")\n        print(f\"  Diagnosis: {res['diagnosis']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:47.741457Z","iopub.execute_input":"2025-04-20T20:35:47.742154Z","iopub.status.idle":"2025-04-20T20:35:53.575187Z","shell.execute_reply.started":"2025-04-20T20:35:47.742130Z","shell.execute_reply":"2025-04-20T20:35:53.574081Z"}},"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n  0%|          | 0/1011 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2326584044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# Train and save everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;31m# Initialize predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2326584044.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepressionDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_bert_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2326584044.py\u001b[0m in \u001b[0;36mextract_bert_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m             inputs = self.tokenizer(batch, return_tensors='tf',\n\u001b[1;32m     74\u001b[0m                                     \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                                     max_length=self.max_length)\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DepressionDetector' object has no attribute 'max_length'"],"ename":"AttributeError","evalue":"'DepressionDetector' object has no attribute 'max_length'","output_type":"error"}],"execution_count":31},{"cell_type":"markdown","source":"","metadata":{}}]}